[
  {
    "id": "prompt_000659",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000700",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000925",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000466",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000099",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000479",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000715",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000095",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000134",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000581",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000476",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000782",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000773",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000080",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000546",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000686",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000886",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000990",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000202",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000340",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000908",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000364",
    "difficulty": "easy",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "string_utils.py test_string_utils.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000631",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000456",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000716",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000706",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000644",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000606",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000610",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000854",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000183",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000401",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] < arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 21",
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000436",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000959",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000055",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000392",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000108",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000842",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000691",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000482",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000029",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000133",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n / factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32",
        "Wrong arithmetic operator on line 38"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000051",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000939",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000553",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000346",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000627",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000904",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000072",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000175",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000250",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000587",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000290",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000823",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000721",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000144",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000277",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000876",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000562",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000358",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000555",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000211",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000104",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000526",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000573",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000637",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000015",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000117",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000280",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000161",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000986",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000279",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000120",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000687",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10",
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000451",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000791",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000326",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000545",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000283",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000293",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000197",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000205",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000328",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000214",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] < arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 21",
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000654",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000889",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000776",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000995",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000023",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000319",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000763",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000674",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000784",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000533",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has a bug in one of the helper functions. The tests are failing. You need to explore the codebase, identify which function is buggy, understand what it should do based on the test expectations, and fix it.",
    "files": [
      {
        "path": "main.py",
        "content": "\"\"\"Multi-module project with issues.\"\"\"\n\n# utils.py functions\ndef helper_one(x):\n    return x + 1  # BUG: Should multiply by 2\n\ndef helper_two(x):\n    return x * 2\n\n# Main functions\ndef process_data(items):\n    result = []\n    for item in items:\n        result.append(helper_one(item))  # Uses buggy function\n    return result\n\ndef calculate_total(values):\n    return sum(values)\n",
        "is_test": false
      },
      {
        "path": "test_main.py",
        "content": "\"\"\"Tests for the module.\"\"\"\nimport pytest\nfrom main import process_data, calculate_total\n\ndef test_process_data():\n    assert process_data([1, 2, 3]) == [2, 4, 6]\n\ndef test_calculate_total():\n    assert calculate_total([1, 2, 3]) == 6\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000105",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000047",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000478",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000341",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000089",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000920",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  }
]
[
  {
    "id": "prompt_000746",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000761",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers)\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers)\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 776 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7",
        "Missing colon on line 11"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000800",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000613",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000039",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000811",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000978",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000400",
    "difficulty": "easy",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "string_utils.py test_string_utils.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000656",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000931",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000861",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000446",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000467",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000302",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000444",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000305",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000977",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000519",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000248",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000578",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000560",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000068",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000247",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000003",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000645",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000212",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000597",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000663",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000167",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000586",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000297",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000961",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000321",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000252",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000795",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000002",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000662",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000634",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000538",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000919",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000653",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000790",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000057",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has a bug in one of the helper functions. The tests are failing. You need to explore the codebase, identify which function is buggy, understand what it should do based on the test expectations, and fix it.",
    "files": [
      {
        "path": "main.py",
        "content": "\"\"\"Multi-module project with issues.\"\"\"\n\n# utils.py functions\ndef helper_one(x):\n    return x + 1  # BUG: Should multiply by 2\n\ndef helper_two(x):\n    return x * 2\n\n# Main functions\ndef process_data(items):\n    result = []\n    for item in items:\n        result.append(helper_one(item))  # Uses buggy function\n    return result\n\ndef calculate_total(values):\n    return sum(values)\n",
        "is_test": false
      },
      {
        "path": "test_main.py",
        "content": "\"\"\"Tests for the module.\"\"\"\nimport pytest\nfrom main import process_data, calculate_total\n\ndef test_process_data():\n    assert process_data([1, 2, 3]) == [2, 4, 6]\n\ndef test_calculate_total():\n    assert calculate_total([1, 2, 3]) == 6\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000269",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000065",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "array_ops.js test_array_ops.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000491",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000866",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000875",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000652",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000976",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000934",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000473",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000673",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000238",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000973",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Fix the broken python code. The issue is straightforward - locate the problem and fix it. Files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "data_processor.py test_data_processor.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000461",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000821",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000245",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000655",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000950",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000628",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000182",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000843",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000720",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000429",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000453",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 != 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Wrong comparison operator on line 9"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000668",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000101",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000219",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000313",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000758",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000488",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) \n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 449 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Missing opening brace on line 11"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000059",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000110",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000259",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000616",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000554",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000792",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000208",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000382",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000863",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000718",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000448",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000910",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000614",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000426",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000376",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000424",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 932 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 16"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000433",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000502",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000576",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000256",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000984",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000928",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000667",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000373",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000449",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000575",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000872",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000164",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000126",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10",
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000149",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000048",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000799",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000186",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000217",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000859",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000671",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000260",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000113",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000998",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000954",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000922",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000712",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000125",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000416",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000807",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000209",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000471",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000847",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000509",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000075",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000658",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000159",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000749",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000603",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000176",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000515",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000601",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has a bug in one of the helper functions. The tests are failing. You need to explore the codebase, identify which function is buggy, understand what it should do based on the test expectations, and fix it.",
    "files": [
      {
        "path": "main.py",
        "content": "\"\"\"Multi-module project with issues.\"\"\"\n\n# utils.py functions\ndef helper_one(x):\n    return x + 1  # BUG: Should multiply by 2\n\ndef helper_two(x):\n    return x * 2\n\n# Main functions\ndef process_data(items):\n    result = []\n    for item in items:\n        result.append(helper_one(item))  # Uses buggy function\n    return result\n\ndef calculate_total(values):\n    return sum(values)\n",
        "is_test": false
      },
      {
        "path": "test_main.py",
        "content": "\"\"\"Tests for the module.\"\"\"\nimport pytest\nfrom main import process_data, calculate_total\n\ndef test_process_data():\n    assert process_data([1, 2, 3]) == [2, 4, 6]\n\ndef test_calculate_total():\n    assert calculate_total([1, 2, 3]) == 6\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000265",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000855",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000839",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000622",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000026",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000143",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000116",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000365",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000064",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000369",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000421",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000419",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000093",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000650",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000501",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000708",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000880",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000806",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000189",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000405",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s)\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 615 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000285",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000733",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000013",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000406",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000532",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000677",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000929",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000409",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000805",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000695",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000158",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000024",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000981",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000303",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000261",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000550",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000649",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000611",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000107",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000957",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000459",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000853",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000647",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000592",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000127",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000964",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000291",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000523",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000337",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000227",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000754",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000767",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000458",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000918",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000069",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000163",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000709",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000619",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000844",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000617",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000121",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000713",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000370",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000816",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000357",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000166",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000719",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000022",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000903",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000253",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 932 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Missing colon on line 3",
        "Missing colon on line 16"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000729",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000543",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000871",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000452",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b)\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 430 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17",
        "Missing colon on line 7"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000040",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000642",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000096",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Fix the broken python code. The issue is straightforward - locate the problem and fix it. Files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "data_processor.py test_data_processor.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000156",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000752",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000936",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000974",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000917",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000010",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000316",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000629",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000913",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000042",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000417",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000477",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000989",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000574",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000106",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000193",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000856",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000070",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000407",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000595",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000085",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 != 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Wrong comparison operator on line 9"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000531",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000151",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000739",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000389",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000257",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000203",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] < arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 21"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000883",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000789",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000418",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000943",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000504",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000953",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000386",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000083",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000379",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000599",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000188",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000744",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000282",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000607",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000764",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000837",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000873",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000865",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000412",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000262",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000204",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000697",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000355",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000058",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000307",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000343",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong comparison operator on line 17",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000087",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000233",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000497",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000777",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 932 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 16"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000937",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000833",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000615",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000972",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000124",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000888",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000540",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000414",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000571",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000760",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000551",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000360",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000971",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000933",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000882",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000234",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000541",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000836",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000060",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000207",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000598",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000621",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000323",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000564",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000300",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000679",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000230",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000320",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000255",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000893",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000503",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000962",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000516",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000850",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000122",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000846",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000437",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000018",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000921",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000310",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000224",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000043",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000751",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000682",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000366",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000745",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000153",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000097",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000798",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000020",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000965",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000589",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000103",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000907",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000235",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000041",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000774",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] < arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 21"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000273",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000788",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000046",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000028",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000881",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000155",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000387",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000830",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000274",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000852",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000874",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000246",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000196",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000342",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000600",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000440",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000381",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000942",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000223",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 8"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000753",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000374",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000549",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000001",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000294",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "array_ops.js test_array_ops.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000757",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000168",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000077",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000896",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000199",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000819",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000997",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000669",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000404",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000334",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000076",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000254",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000009",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000443",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000438",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000292",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000762",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000530",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000832",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000563",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000385",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000226",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000732",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000329",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000314",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000428",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000220",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000276",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) \n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 449 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Missing opening brace on line 11"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000638",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The python project has failing tests. The issue is straightforward - locate the problem and fix it. Files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "algorithms.py test_algorithms.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000577",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000955",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000330",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000468",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000201",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000511",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000102",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000770",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000035",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000225",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000786",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000585",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "string_utils.py test_string_utils.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000132",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000781",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000170",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000198",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000500",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000664",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000402",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000912",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000082",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10",
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000131",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) \n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 449 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Missing opening brace on line 11",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000165",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000397",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000696",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000897",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000354",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000218",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000975",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000141",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000232",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000327",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000661",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000785",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "array_ops.js test_array_ops.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000514",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000699",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000737",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000698",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000772",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000056",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000940",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000411",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000812",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000331",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000657",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000430",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000648",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000780",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000759",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000044",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000061",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000988",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000566",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000602",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000815",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000714",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000826",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000336",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000304",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong comparison operator on line 17",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000840",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000895",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "data_processor.py test_data_processor.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000802",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000012",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000017",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000098",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s)\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 615 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7",
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000980",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000680",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000371",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000666",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000542",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000228",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000738",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000624",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000268",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000593",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n / factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32",
        "Wrong arithmetic operator on line 38"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000688",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000505",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000742",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000916",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b)\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 430 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17",
        "Missing colon on line 7"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000905",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() - str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8",
        "Wrong arithmetic operator on line 16"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000625",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000005",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000901",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000160",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000705",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000525",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000845",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000172",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000567",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000829",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000394",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000572",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000344",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000963",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000123",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000946",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000345",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000395",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000236",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000689",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000299",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000736",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000675",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000050",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000756",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000136",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000138",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000768",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000633",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000439",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000723",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000911",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000312",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000951",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Fix the broken python code. The issue is straightforward - locate the problem and fix it. Files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "data_processor.py test_data_processor.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000432",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000181",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000857",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] < arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 21"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000272",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000469",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000049",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000021",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000489",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000349",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000958",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000492",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000869",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000982",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000195",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a / b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong arithmetic operator on line 13"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000924",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000991",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000724",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000636",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000006",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000704",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000884",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000271",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000074",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000808",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000485",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "algorithms.py test_algorithms.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000693",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000711",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000239",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000556",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000308",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000487",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000408",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000475",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000527",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000053",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000906",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000270",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n)\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 931 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 16",
        "Missing colon on line 25"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000834",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000353",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000215",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000570",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000450",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000956",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000835",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 != 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Missing colon on line 3",
        "Wrong comparison operator on line 9"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000422",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000507",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000643",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000148",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000632",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000582",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000033",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000185",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000900",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000735",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000267",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000948",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000672",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000690",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000154",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000178",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000947",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000014",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000263",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000281",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000985",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000915",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000258",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000284",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000078",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000517",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000129",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000084",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n / factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 38"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000496",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b)\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 430 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000879",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000027",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000694",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "string_utils.py test_string_utils.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000052",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000173",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000484",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "algorithms.py test_algorithms.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000825",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000266",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000684",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000036",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000171",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000296",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000891",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000361",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000431",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000993",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000579",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000524",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000783",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000480",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000639",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000979",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000351",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000670",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000820",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000552",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong arithmetic operator on line 8"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000765",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000483",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000251",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000090",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000535",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000813",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000529",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000797",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000286",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000539",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000145",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000081",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000748",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000702",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000778",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000377",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000941",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000809",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000944",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000034",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000375",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000590",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000559",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a / b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong arithmetic operator on line 13",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000420",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000275",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000513",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000242",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000510",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000838",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000007",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000623",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b)\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 430 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Missing colon on line 3",
        "Missing colon on line 7"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000190",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000231",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000363",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000646",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000460",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000565",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000434",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000521",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000465",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000184",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000435",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000771",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000945",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The JavaScript code has a bug that's causing test failures. Search through the code to find the issue and fix it.",
    "files": [
      {
        "path": "main.js",
        "content": "// Multi-function utility module\n\nfunction helperOne(x) {\n    return x + 1;  // BUG: Should multiply by 2\n}\n\nfunction helperTwo(x) {\n    return x * 2;\n}\n\nfunction processData(items) {\n    return items.map(helperOne);\n}\n\nmodule.exports = { processData, helperOne, helperTwo };\n",
        "is_test": false
      },
      {
        "path": "test_main.js",
        "content": "const { processData } = require('./main');\n\nfunction test_processData() {\n    const result = processData([1, 2, 3]);\n    const expected = [2, 4, 6];\n    if (JSON.stringify(result) !== JSON.stringify(expected)) {\n        throw new Error(`Expected ${expected}, got ${result}`);\n    }\n    console.log(\"\u2713 test_processData passed\");\n}\n\ntest_processData();\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat main.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_main.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "grep_intensive",
      "command_focus": "grep",
      "solution_steps": [
        "Find all function definitions: grep -n 'def' main.py (or 'function' for JS)",
        "Search for the buggy helper usage: grep -r 'helper_one' .",
        "Check the test expectations: grep 'assert' test file",
        "Identify that helper_one adds instead of multiplies",
        "Fix with sed: sed -i 's/x + 1/x * 2/g' main file",
        "Run tests to verify fix"
      ]
    }
  },
  {
    "id": "prompt_000425",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000140",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 932 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 16"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000086",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000139",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000447",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000817",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000793",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000016",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000073",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000011",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s)\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s)\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 614 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7",
        "Missing colon on line 12"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000137",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000128",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000295",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) \n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 450 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000932",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000427",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000368",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000747",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000966",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000192",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000902",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000779",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "data_processor.py test_data_processor.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000734",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000216",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000558",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000683",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000390",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000717",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000391",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000325",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000640",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000403",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000755",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 17",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000442",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000240",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000983",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000288",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000112",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000367",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000864",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. The issue is straightforward - locate the problem and fix it. Files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "array_ops.js test_array_ops.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000969",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000054",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000858",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000378",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000063",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] != target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000301",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000968",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000413",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000522",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000626",
    "difficulty": "easy",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000008",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000596",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5",
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000091",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a / b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() - str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 8",
        "Wrong arithmetic operator on line 16"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000352",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000740",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 431 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000311",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000583",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000306",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000324",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000914",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000200",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000457",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target):\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 934 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000887",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000630",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000399",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000298",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000544",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000463",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Fix the broken python code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr)\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 932 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 16"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000870",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000707",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000150",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000000",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000651",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 != 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong comparison operator on line 5",
        "Wrong comparison operator on line 9"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000594",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000142",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The text processor has a bug - it's treating words with different cases as different words (e.g., 'apple' and 'Apple'). Fix the code to handle case-insensitive processing.",
    "files": [
      {
        "path": "words.txt",
        "content": "apple\nbanana\napple\nCherry\nbanana\nApple\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "def process():\n    # BUG: Case-sensitive duplicates\n    with open('words.txt') as f:\n        return list(set(f.read().split()))\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat words.txt"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "processor.py",
        "description": "Uses lowercase"
      }
    ],
    "metadata": {
      "scenario_type": "text_transform",
      "command_focus": "tr,sort,uniq",
      "solution_steps": [
        "View file: cat words.txt",
        "Convert to lowercase: cat words.txt | tr 'A-Z' 'a-z'",
        "Sort: cat words.txt | tr 'A-Z' 'a-z' | sort",
        "Get unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq",
        "Count unique: cat words.txt | tr 'A-Z' 'a-z' | sort | uniq | wc -l",
        "Identify issue: code doesn't lowercase",
        "Fix: sed -i 's/f.read().split()/f.read().lower().split()/g' processor.py"
      ]
    }
  },
  {
    "id": "prompt_000222",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000092",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s):\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 617 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000062",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000676",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000660",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !!= 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000506",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000923",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000388",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000725",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000994",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000111",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Refactor the codebase: rename 'old_function' to 'new_function' everywhere it appears. Make sure to update it in all files where it's used.",
    "files": [
      {
        "path": "module1.py",
        "content": "def old_function():\n    return \"old\"\n",
        "is_test": false
      },
      {
        "path": "module2.py",
        "content": "from module1 import old_function\n\ndef caller():\n    return old_function()\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "module1.py",
        "description": "Renamed"
      }
    ],
    "metadata": {
      "scenario_type": "refactoring",
      "command_focus": "find,xargs,sed",
      "solution_steps": [
        "Find all occurrences: grep -r 'old_function' .",
        "Find all Python files: find . -name '*.py'",
        "Rename in module1: sed -i 's/old_function/new_function/g' module1.py",
        "Rename in module2: sed -i 's/old_function/new_function/g' module2.py",
        "Or use find+xargs: find . -name '*.py' -exec sed -i 's/old_function/new_function/g' {} \\;",
        "Verify: grep -r 'new_function' ."
      ]
    }
  },
  {
    "id": "prompt_000612",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000071",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000766",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000967",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000180",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000455",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000804",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000999",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !!= 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000890",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "validators.js test_validators.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000474",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5",
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000828",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The calculator module has multiple bugs that are causing test failures. There are also debug statements that should be removed. Find and fix all issues to make the tests pass.",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Module with multiple bugs.\"\"\"\n\ndef calculate(a, b):\n    # BUG 1: Wrong operator\n    result = a - b  # Should be a + b\n    return result\n\ndef multiply(x, y):\n    # BUG 2: Wrong operator\n    return x + y  # Should be x * y\n\ndef divide(a, b):\n    # BUG 3: Missing check\n    return a / b  # Should check b != 0\n\n# DEBUG CODE TO REMOVE\nprint(\"DEBUG: Loading module\")\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "import pytest\nfrom calculator import calculate, multiply, divide\n\ndef test_calculate():\n    assert calculate(5, 3) == 8\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n\ndef test_divide():\n    assert divide(10, 2) == 5\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000827",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000867",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000498",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Fix the broken python code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b)\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b)\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b)\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 429 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7",
        "Missing colon on line 11"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000851",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all .txt files in the 'data' directory that contain 'TODO' and create a report.txt file listing the filenames and the count of TODO items in each file.",
    "files": [
      {
        "path": "data/file1.txt",
        "content": "TODO: Review this\nSome content\nFIXME: Bug here",
        "is_test": false
      },
      {
        "path": "data/file2.txt",
        "content": "Clean content\nNo issues",
        "is_test": false
      },
      {
        "path": "data/file3.txt",
        "content": "TODO: Update docs\nMore content",
        "is_test": false
      },
      {
        "path": "other/notes.txt",
        "content": "TODO: Remember this",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls data/"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "report.txt",
        "description": "Report created"
      }
    ],
    "metadata": {
      "scenario_type": "batch_processing",
      "command_focus": "find,xargs,grep",
      "solution_steps": [
        "Find txt files in data: find data/ -name '*.txt'",
        "Search for TODO in each: find data/ -name '*.txt' -exec grep -l 'TODO' {} \\;",
        "Count TODOs per file: find data/ -name '*.txt' -exec sh -c 'echo \"{}:\" $(grep -c TODO {})' \\;",
        "Or use xargs: find data/ -name '*.txt' | xargs grep -c TODO",
        "Create report: find data/ -name '*.txt' -exec grep -c TODO {} + > report.txt",
        "Better format: find data/ -name '*.txt' -print0 | xargs -0 grep -l TODO | tee report.txt",
        "Verify: cat report.txt"
      ]
    }
  },
  {
    "id": "prompt_000728",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000079",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000685",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000678",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000177",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000241",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000743",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Reorganize the project structure by moving utils.py into a new 'lib' directory and updating imports accordingly. Make sure the code still runs after the reorganization.",
    "files": [
      {
        "path": "utils.py",
        "content": "def utility_function():\n    return \"util\"\n",
        "is_test": false
      },
      {
        "path": "main.py",
        "content": "from utils import utility_function\n\ndef main():\n    print(utility_function())\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "execution",
        "target": "main.py",
        "description": "Code should run"
      }
    ],
    "metadata": {
      "scenario_type": "file_ops",
      "command_focus": "cp,mv,mkdir",
      "solution_steps": [
        "Create directory: mkdir lib",
        "Copy file: cp utils.py lib/utils.py",
        "Update import: sed -i 's/from utils/from lib.utils/g' main.py",
        "Make package: touch lib/__init__.py",
        "Verify structure: ls lib/",
        "Check import: cat main.py | grep import",
        "Test execution: python main.py"
      ]
    }
  },
  {
    "id": "prompt_000557",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000350",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Initialize a git repository, commit the initial feature.py file, then update the version string from 'v1' to 'v2' and commit that change. Track your work with git throughout.",
    "files": [
      {
        "path": "feature.py",
        "content": "def feature():\n    return \"v1\"  # Update to v2\n",
        "is_test": false
      }
    ],
    "cli_history": [],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "feature.py",
        "description": "Version updated"
      }
    ],
    "metadata": {
      "scenario_type": "git",
      "command_focus": "git",
      "solution_steps": [
        "Initialize repo: git init",
        "Stage file: git add feature.py",
        "Initial commit: git commit -m 'Initial commit'",
        "Make change: sed -i 's/v1/v2/g' feature.py",
        "View changes: git diff",
        "Stage changes: git add feature.py",
        "Commit update: git commit -m 'Update to v2'",
        "View history: git log --oneline"
      ]
    }
  },
  {
    "id": "prompt_000580",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) \n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 511 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 8"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000445",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Analyze the web server access logs and create a summary.txt file that reports the count of errors (status codes 404 and 500). Explore the log file to understand its format first.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.1 - - [30/Oct/2024:10:00:00] \"GET /api/users HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:01] \"POST /api/data HTTP/1.1\" 201\n192.168.1.1 - - [30/Oct/2024:10:00:05] \"GET /api/users HTTP/1.1\" 404\n192.168.1.3 - - [30/Oct/2024:10:00:10] \"GET /api/items HTTP/1.1\" 200\n192.168.1.2 - - [30/Oct/2024:10:00:15] \"DELETE /api/data HTTP/1.1\" 500\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head -3 access.log"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "summary.txt",
        "description": "Summary created"
      }
    ],
    "metadata": {
      "scenario_type": "log_analysis",
      "command_focus": "awk,cut,grep,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Find errors: grep '404\\|500' access.log",
        "Extract IPs: cut -d' ' -f1 access.log | sort | uniq",
        "Count status codes: awk '{print $9}' access.log | sort | uniq -c",
        "Count GET requests: grep 'GET' access.log | wc -l",
        "Extract paths: cat access.log | cut -d'\"' -f2 | cut -d' ' -f2",
        "Create summary: echo \"Error count: $(grep -c '404\\|500' access.log)\" > summary.txt"
      ]
    }
  },
  {
    "id": "prompt_000441",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000605",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000710",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000996",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 12",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000930",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Fix the broken python code. The issue is straightforward - locate the problem and fix it. Files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "calculator.py test_calculator.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000987",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000393",
    "difficulty": "medium",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers):\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n < 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 779 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 5"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000493",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000162",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The python project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 32",
        "Missing colon on line 3"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000561",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000249",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000635",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: algorithms.py, test_algorithms.py",
    "files": [
      {
        "path": "algorithms.py",
        "content": "\"\"\"Algorithm implementations.\"\"\"\n\ndef binary_search(arr, target)\n    \"\"\"Binary search in sorted array.\"\"\"\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef bubble_sort(arr):\n    \"\"\"Sort array using bubble sort.\"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\ndef fibonacci(n):\n    \"\"\"Calculate nth Fibonacci number.\"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) - fibonacci(n - 2)\n\ndef factorial(n):\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n",
        "is_test": false
      },
      {
        "path": "test_algorithms.py",
        "content": "\"\"\"Tests for algorithms.\"\"\"\n\nfrom algorithms import binary_search, bubble_sort, fibonacci, factorial\n\ndef test_binary_search():\n    assert binary_search([1, 2, 3, 4, 5], 3) == 2\n    assert binary_search([1, 2, 3, 4, 5], 6) == -1\n    assert binary_search([1], 1) == 0\n\ndef test_bubble_sort():\n    assert bubble_sort([3, 1, 4, 1, 5]) == [1, 1, 3, 4, 5]\n    assert bubble_sort([]) == []\n    assert bubble_sort([1]) == [1]\n\ndef test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n\ndef test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 933 Oct 30 10:00 algorithms.py",
      "-rw-r--r-- 1 user user 672 Oct 30 10:00 test_algorithms.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_algorithms.py",
        "description": "All algorithm tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Wrong arithmetic operator on line 32"
      ],
      "scenario_type": "algorithms"
    }
  },
  {
    "id": "prompt_000244",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000960",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000604",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000410",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s):\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned != cleaned[::-1]\n\ndef count_vowels(s):\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 616 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 10",
        "Missing colon on line 3"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000775",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000877",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000949",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000848",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4",
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000665",
    "difficulty": "easy",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. The issue is straightforward - locate the problem and fix it. Files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "calculator.py test_calculator.py"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000569",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Run the script and separate the output: save standard output to 'output.log', errors to 'errors.log', and create a combined log 'all.log' with both. Verify all three files exist with the correct content.",
    "files": [
      {
        "path": "script.py",
        "content": "#!/usr/bin/env python3\nimport sys\n\nprint(\"Standard output message\")\nprint(\"Error message\", file=sys.stderr)\nprint(\"Another output\")\nprint(\"Another error\", file=sys.stderr)\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat script.py"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "output.log",
        "description": "Output log created"
      },
      {
        "type": "text_match",
        "target": "errors.log",
        "description": "Error log created"
      }
    ],
    "metadata": {
      "scenario_type": "redirection",
      "command_focus": "redirection,pipes",
      "solution_steps": [
        "Run with stdout redirect: python3 script.py > output.log",
        "Run with stderr redirect: python3 script.py 2> errors.log",
        "Run with both: python3 script.py > output.log 2> errors.log",
        "Run with combined: python3 script.py &> all.log",
        "Or: python3 script.py > all.log 2>&1",
        "Verify output: cat output.log",
        "Verify errors: cat errors.log",
        "Verify combined: cat all.log"
      ]
    }
  },
  {
    "id": "prompt_000333",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000109",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 548 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000731",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000415",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length === 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000927",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000520",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000130",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000588",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) \n    return arr.reduce((a, b) => a + b, 0);\n}\n\nfunction findMax(arr) \n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 547 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js",
      "$ node test_array_ops.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Missing opening brace on line 7",
        "Wrong comparison operator on line 8"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000618",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The calculator module has bugs causing test failures. There are also debug statements that need to be removed. Fix all the issues.",
    "files": [
      {
        "path": "calculator.js",
        "content": "// Module with bugs\n\nfunction calculate(a, b) {\n    // BUG: Wrong operator\n    return a - b;  // Should be a + b\n}\n\nfunction multiply(x, y) {\n    // BUG: Wrong operator\n    return x + y;  // Should be x * y\n}\n\n// DEBUG\nconsole.log(\"DEBUG: Loading\");\n\nmodule.exports = { calculate, multiply };\n",
        "is_test": false
      },
      {
        "path": "test_calculator.js",
        "content": "const { calculate, multiply } = require('./calculator');\n\nif (calculate(5, 3) !== 8) throw new Error(\"calculate failed\");\nif (multiply(4, 5) !== 20) throw new Error(\"multiply failed\");\nconsole.log(\"\u2713 All tests passed\");\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat calculator.*"
    ],
    "expected_commands": 5,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.js",
        "description": "Tests must pass"
      }
    ],
    "metadata": {
      "scenario_type": "sed_intensive",
      "command_focus": "sed",
      "solution_steps": [
        "Fix subtract to add: sed -i 's/a - b/a + b/g' calculator file",
        "Fix add to multiply: sed -i 's/x + y/x * y/g' calculator file",
        "Remove debug lines: sed -i '/DEBUG/d' calculator file",
        "Add zero check for divide (Python): sed -i '/return a / b/i\\    if b == 0: raise ValueError(...)' calculator.py",
        "Run tests to verify all fixes"
      ]
    }
  },
  {
    "id": "prompt_000356",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Compare the two fruit files and create a merged.txt file that contains all unique fruits from both files (no duplicates).",
    "files": [
      {
        "path": "fruits1.txt",
        "content": "apple\nbanana\ncherry\n",
        "is_test": false
      },
      {
        "path": "fruits2.txt",
        "content": "apple\nblueberry\ncherry\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.txt"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "merged.txt",
        "description": "Has new fruit"
      }
    ],
    "metadata": {
      "scenario_type": "comparison",
      "command_focus": "diff,comm",
      "solution_steps": [
        "Compare files: diff fruits1.txt fruits2.txt",
        "Unified format: diff -u fruits1.txt fruits2.txt",
        "Common lines: comm fruits1.txt fruits2.txt",
        "Merge and deduplicate: cat fruits1.txt fruits2.txt | sort | uniq > merged.txt",
        "Verify result: cat merged.txt"
      ]
    }
  },
  {
    "id": "prompt_000289",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Fix the broken python code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000174",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 !== 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3",
        "Wrong comparison operator on line 12"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000472",
    "difficulty": "easy",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. The issue is straightforward - locate the problem and fix it. Files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls",
      "utils.js test_utils.js"
    ],
    "expected_commands": 3,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000490",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000359",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "The log analyzer is failing tests. It should count error messages in the log file, but it's returning the wrong value. Examine the log file and the code to understand what's wrong and fix it.",
    "files": [
      {
        "path": "server.log",
        "content": "2024-10-30 10:00:00 INFO Server started\n2024-10-30 10:00:01 INFO Connected to database\n2024-10-30 10:00:05 ERROR Failed to load config\n2024-10-30 10:00:10 WARNING Retry attempt 1\n2024-10-30 10:00:15 ERROR Connection timeout\n2024-10-30 10:00:20 INFO Request processed\n2024-10-30 10:00:25 ERROR Database query failed\n",
        "is_test": false
      },
      {
        "path": "analyzer.py",
        "content": "\"\"\"Log analyzer with bug.\"\"\"\n\ndef count_errors(filename):\n    # BUG: Counts all lines, not just errors\n    count = 0\n    with open(filename) as f:\n        for line in f:\n            count += 1  # Should filter for ERROR\n    return count\n",
        "is_test": false
      },
      {
        "path": "test_analyzer.py",
        "content": "from analyzer import count_errors\n\ndef test_count():\n    result = count_errors('server.log')\n    assert result == 3, f\"Expected 3 errors, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "head server.log"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_analyzer.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "piping",
      "command_focus": "pipes",
      "solution_steps": [
        "View log: cat server.log",
        "Filter errors: cat server.log | grep ERROR",
        "Count errors: cat server.log | grep ERROR | wc -l",
        "Check log levels: cat server.log | cut -d' ' -f4 | sort | uniq",
        "Identify bug: code counts all lines instead of ERROR lines",
        "Fix: sed -i 's/count += 1/if \"ERROR\" in line:\\n                count += 1/g' analyzer.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000115",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Process the access log to create 'top_ips.txt' containing the top 3 IP addresses by request count, sorted by frequency. Each line should show the count and IP address.",
    "files": [
      {
        "path": "access.log",
        "content": "192.168.1.10 - - [30/Oct/2024:10:15:23] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:24] \"POST /api/login HTTP/1.1\" 200 567\n192.168.1.10 - - [30/Oct/2024:10:15:25] \"GET /api/profile HTTP/1.1\" 200 890\n192.168.1.12 - - [30/Oct/2024:10:15:26] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.11 - - [30/Oct/2024:10:15:27] \"DELETE /api/session HTTP/1.1\" 204 0\n192.168.1.10 - - [30/Oct/2024:10:15:28] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "head access.log"
    ],
    "expected_commands": 10,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "top_ips.txt",
        "description": "Top IPs listed"
      }
    ],
    "metadata": {
      "scenario_type": "data_pipeline",
      "command_focus": "cut,sort,uniq,head,pipes",
      "solution_steps": [
        "View log: cat access.log",
        "Extract IPs: cut -d' ' -f1 access.log",
        "Sort IPs: cut -d' ' -f1 access.log | sort",
        "Count occurrences: cut -d' ' -f1 access.log | sort | uniq -c",
        "Sort by count: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn",
        "Get top 3: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3",
        "Save to file: cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -3 > top_ips.txt",
        "Verify: cat top_ips.txt"
      ]
    }
  },
  {
    "id": "prompt_000486",
    "difficulty": "hard",
    "language": "python",
    "task_description": "There's a bug in the python code that needs fixing. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: calculator.py, test_calculator.py",
    "files": [
      {
        "path": "calculator.py",
        "content": "\"\"\"Simple calculator module.\"\"\"\n\ndef add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a - b\n\ndef subtract(a, b):\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"Divide a by b.\"\"\"\n    if b != 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef power(a, b):\n    \"\"\"Calculate a to the power of b.\"\"\"\n    return a ** b\n",
        "is_test": false
      },
      {
        "path": "test_calculator.py",
        "content": "\"\"\"Tests for calculator module.\"\"\"\n\nimport pytest\nfrom calculator import add, subtract, multiply, divide, power\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\n    assert subtract(0, 5) == -5\n\ndef test_multiply():\n    assert multiply(4, 5) == 20\n    assert multiply(-2, 3) == -6\n\ndef test_divide():\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    with pytest.raises(ValueError):\n        divide(5, 0)\n\ndef test_power():\n    assert power(2, 3) == 8\n    assert power(5, 0) == 1\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 432 Oct 30 10:00 calculator.py",
      "-rw-r--r-- 1 user user 593 Oct 30 10:00 test_calculator.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_calculator.py",
        "description": "All calculator tests must pass"
      },
      {
        "type": "lint",
        "target": "calculator.py",
        "description": "Code must pass basic linting"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 17",
        "Wrong arithmetic operator on line 5"
      ],
      "scenario_type": "calculator"
    }
  },
  {
    "id": "prompt_000066",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: array_ops.js, test_array_ops.js",
    "files": [
      {
        "path": "array_ops.js",
        "content": "// Array operation functions\n\nfunction sum(arr) {\n    return arr.reduce((a, b) => a - b, 0);\n}\n\nfunction findMax(arr) {\n    if (arr.length !== 0) return null;\n    return Math.max(...arr);\n}\n\nfunction removeDuplicates(arr) {\n    return [...new Set(arr)];\n}\n\nfunction flatten(arr) {\n    return arr.flat();\n}\n\nfunction chunk(arr, size) {\n    const result = [];\n    for (let i = 0; i < arr.length; i += size) {\n        result.push(arr.slice(i, i + size));\n    }\n    return result;\n}\n\nmodule.exports = { sum, findMax, removeDuplicates, flatten, chunk };\n",
        "is_test": false
      },
      {
        "path": "test_array_ops.js",
        "content": "// Tests for array operations\n\nconst { sum, findMax, removeDuplicates, flatten, chunk } = require('./array_ops');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_sum() {\n    assertEquals(sum([1, 2, 3, 4, 5]), 15, \"sum([1, 2, 3, 4, 5])\");\n    assertEquals(sum([]), 0, \"sum([])\");\n    console.log(\"\u2713 test_sum passed\");\n}\n\nfunction test_findMax() {\n    assertEquals(findMax([1, 5, 3, 9, 2]), 9, \"findMax([1, 5, 3, 9, 2])\");\n    assertEquals(findMax([]), null, \"findMax([])\");\n    console.log(\"\u2713 test_findMax passed\");\n}\n\nfunction test_removeDuplicates() {\n    assertEquals(removeDuplicates([1, 2, 2, 3, 1, 4]), [1, 2, 3, 4], \"removeDuplicates\");\n    console.log(\"\u2713 test_removeDuplicates passed\");\n}\n\nfunction test_flatten() {\n    assertEquals(flatten([[1, 2], [3, 4], [5]]), [1, 2, 3, 4, 5], \"flatten\");\n    console.log(\"\u2713 test_flatten passed\");\n}\n\nfunction test_chunk() {\n    assertEquals(chunk([1, 2, 3, 4, 5], 2), [[1, 2], [3, 4], [5]], \"chunk\");\n    console.log(\"\u2713 test_chunk passed\");\n}\n\n// Run all tests\ntry {\n    test_sum();\n    test_findMax();\n    test_removeDuplicates();\n    test_flatten();\n    test_chunk();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 549 Oct 30 10:00 array_ops.js",
      "-rw-r--r-- 1 user user 1435 Oct 30 10:00 test_array_ops.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_array_ops.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong comparison operator on line 8",
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "array_ops"
    }
  },
  {
    "id": "prompt_000030",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a - b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Wrong arithmetic operator on line 4"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000499",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Update the configuration file: enable DEBUG mode, change LOG_LEVEL to 'debug', enable CACHE, update API_KEY to 'new_key_67890', and add a comment '# Updated for development' at the top.",
    "files": [
      {
        "path": "config.env",
        "content": "# Application Configuration\nDEBUG=false\nLOG_LEVEL=info\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\n# API Configuration\nAPI_KEY=old_key_12345\nAPI_TIMEOUT=30\n# Cache Settings\nCACHE_ENABLED=false\nCACHE_TTL=3600\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "cat config.env"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "config.env",
        "description": "DEBUG enabled"
      },
      {
        "type": "text_match",
        "target": "config.env",
        "description": "API key updated"
      }
    ],
    "metadata": {
      "scenario_type": "config_editing",
      "command_focus": "sed,grep",
      "solution_steps": [
        "Backup: cp config.env config.env.bak",
        "Enable DEBUG: sed -i 's/DEBUG=false/DEBUG=true/g' config.env",
        "Change LOG_LEVEL: sed -i 's/LOG_LEVEL=info/LOG_LEVEL=debug/g' config.env",
        "Enable CACHE: sed -i 's/CACHE_ENABLED=false/CACHE_ENABLED=true/g' config.env",
        "Update API_KEY: sed -i 's/API_KEY=old_key_12345/API_KEY=new_key_67890/g' config.env",
        "Add comment at top: sed -i '1i# Updated for development' config.env",
        "Verify changes: cat config.env",
        "Or check specific: grep -E 'DEBUG|LOG_LEVEL|CACHE_ENABLED|API_KEY' config.env"
      ]
    }
  },
  {
    "id": "prompt_000464",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000332",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "The javascript project has failing tests. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) \n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 451 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000338",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Make the deploy.sh and deploy.py scripts executable. The README should remain read-only. Verify the permissions are set correctly.",
    "files": [
      {
        "path": "deploy.sh",
        "content": "#!/bin/bash\necho \"Running deployment script...\"\n",
        "is_test": false
      },
      {
        "path": "deploy.py",
        "content": "#!/usr/bin/env python3\nprint(\"Deploying application...\")\n",
        "is_test": false
      },
      {
        "path": "README.md",
        "content": "# Deployment Scripts\n\nRun deploy.sh to start deployment.\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls -l"
    ],
    "expected_commands": 7,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Scripts exist"
      }
    ],
    "metadata": {
      "scenario_type": "permissions",
      "command_focus": "chmod,ls",
      "solution_steps": [
        "Check current permissions: ls -l",
        "Make deploy.sh executable: chmod +x deploy.sh",
        "Make deploy.py executable: chmod +x deploy.py",
        "Or both at once: chmod +x deploy.*",
        "Make README read-only: chmod 444 README.md",
        "Verify: ls -l",
        "Test execution: ./deploy.sh"
      ]
    }
  },
  {
    "id": "prompt_000681",
    "difficulty": "very_hard",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. This is a complex debugging task with multiple related issues. You'll need to understand the architecture and trace dependencies. Start by running tests to see what's failing. Files in project: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 12,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000824",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000187",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The python project has failing tests. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: string_utils.py, test_string_utils.py",
    "files": [
      {
        "path": "string_utils.py",
        "content": "\"\"\"String utility functions.\"\"\"\n\ndef reverse_string(s)\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\ndef is_palindrome(s)\n    \"\"\"Check if string is a palindrome.\"\"\"\n    cleaned = s.lower().replace(\" \", \"\")\n    return cleaned == cleaned[::-1]\n\ndef count_vowels(s)\n    \"\"\"Count vowels in a string.\"\"\"\n    vowels = \"aeiouAEIOU\"\n    return sum(1 for char in s if char in vowels)\n\ndef capitalize_words(s):\n    \"\"\"Capitalize first letter of each word.\"\"\"\n    return \" \".join(word.capitalize() for word in s.split())\n\ndef remove_whitespace(s):\n    \"\"\"Remove all whitespace from string.\"\"\"\n    return \"\".join(s.split())\n",
        "is_test": false
      },
      {
        "path": "test_string_utils.py",
        "content": "\"\"\"Tests for string utilities.\"\"\"\n\nfrom string_utils import reverse_string, is_palindrome, count_vowels, capitalize_words, remove_whitespace\n\ndef test_reverse_string():\n    assert reverse_string(\"hello\") == \"olleh\"\n    assert reverse_string(\"\") == \"\"\n    assert reverse_string(\"a\") == \"a\"\n\ndef test_is_palindrome():\n    assert is_palindrome(\"racecar\") == True\n    assert is_palindrome(\"hello\") == False\n    assert is_palindrome(\"A man a plan a canal Panama\") == True\n\ndef test_count_vowels():\n    assert count_vowels(\"hello\") == 2\n    assert count_vowels(\"AEIOU\") == 5\n    assert count_vowels(\"xyz\") == 0\n\ndef test_capitalize_words():\n    assert capitalize_words(\"hello world\") == \"Hello World\"\n    assert capitalize_words(\"python programming\") == \"Python Programming\"\n\ndef test_remove_whitespace():\n    assert remove_whitespace(\"hello world\") == \"helloworld\"\n    assert remove_whitespace(\"  a  b  c  \") == \"abc\"\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 614 Oct 30 10:00 string_utils.py",
      "-rw-r--r-- 1 user user 913 Oct 30 10:00 test_string_utils.py",
      "$ pytest -v",
      "test_*.py::test_* FAILED",
      "Some tests are failing..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_string_utils.py",
        "description": "All string utility tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3",
        "Missing colon on line 7",
        "Missing colon on line 12"
      ],
      "scenario_type": "string_utils"
    }
  },
  {
    "id": "prompt_000831",
    "difficulty": "hard",
    "language": "python",
    "task_description": "The CSV processor is failing tests. The program processes a CSV file but seems to have an issue with how it reads the data. Investigate the data file structure and fix the processor to handle it correctly.",
    "files": [
      {
        "path": "data.csv",
        "content": "name,age,score\nAlice,25,85\nBob,30,92\nCharlie,22,78\nDavid,35,95\n",
        "is_test": false
      },
      {
        "path": "processor.py",
        "content": "\"\"\"Process CSV data.\"\"\"\n\ndef process_data(filename):\n    # BUG: Doesn't handle header correctly\n    total = 0\n    with open(filename) as f:\n        for line in f:\n            parts = line.strip().split(',')\n            total += int(parts[2])  # Will fail on header\n    return total\n",
        "is_test": false
      },
      {
        "path": "test_processor.py",
        "content": "from processor import process_data\n\ndef test_process():\n    result = process_data('data.csv')\n    assert result == 350, f\"Expected 350, got {result}\"\n    print(\"\u2713 Test passed\")\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "ls",
      "cat data.csv | head -3"
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_processor.py",
        "description": "Test must pass"
      }
    ],
    "metadata": {
      "scenario_type": "awk_cut",
      "command_focus": "awk,cut",
      "solution_steps": [
        "Examine data file: cat data.csv",
        "See header: head -n 1 data.csv",
        "See data without header: tail -n +2 data.csv",
        "Extract scores column: cut -d',' -f3 data.csv",
        "Extract scores without header: awk -F',' 'NR>1 {print $3}' data.csv",
        "Identify bug: processor doesn't skip header line",
        "Fix by adding skip: sed -i '/for line in f:/a\\        next(f)  # Skip header' processor.py",
        "Run test to verify"
      ]
    }
  },
  {
    "id": "prompt_000372",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "The code has issues that prevent it from working correctly. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: utils.js, test_utils.js",
    "files": [
      {
        "path": "utils.js",
        "content": "// Utility functions\n\nfunction add(a, b) {\n    return a + b;\n}\n\nfunction multiply(a, b) {\n    return a * b;\n}\n\nfunction isEven(n) {\n    return n % 2 === 0;\n}\n\nfunction capitalize(str) {\n    return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nfunction range(start, end) {\n    const result = [];\n    for (let i = start; i <= end; i++) {\n        result.push(i);\n    }\n    return result;\n}\n\nmodule.exports = { add, multiply, isEven, capitalize, range };\n",
        "is_test": false
      },
      {
        "path": "test_utils.js",
        "content": "// Tests for utility functions\n\nconst { add, multiply, isEven, capitalize, range } = require('./utils');\n\nfunction assertEquals(actual, expected, message) {\n    if (JSON.stringify(actual) !== JSON.stringify(expected)) {\n        throw new Error(`${message}: expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);\n    }\n}\n\nfunction test_add() {\n    assertEquals(add(2, 3), 5, \"add(2, 3)\");\n    assertEquals(add(-1, 1), 0, \"add(-1, 1)\");\n    console.log(\"\u2713 test_add passed\");\n}\n\nfunction test_multiply() {\n    assertEquals(multiply(4, 5), 20, \"multiply(4, 5)\");\n    assertEquals(multiply(-2, 3), -6, \"multiply(-2, 3)\");\n    console.log(\"\u2713 test_multiply passed\");\n}\n\nfunction test_isEven() {\n    assertEquals(isEven(4), true, \"isEven(4)\");\n    assertEquals(isEven(5), false, \"isEven(5)\");\n    console.log(\"\u2713 test_isEven passed\");\n}\n\nfunction test_capitalize() {\n    assertEquals(capitalize(\"hello\"), \"Hello\", \"capitalize('hello')\");\n    assertEquals(capitalize(\"world\"), \"World\", \"capitalize('world')\");\n    console.log(\"\u2713 test_capitalize passed\");\n}\n\nfunction test_range() {\n    assertEquals(range(1, 5), [1, 2, 3, 4, 5], \"range(1, 5)\");\n    assertEquals(range(0, 0), [0], \"range(0, 0)\");\n    console.log(\"\u2713 test_range passed\");\n}\n\n// Run all tests\ntry {\n    test_add();\n    test_multiply();\n    test_isEven();\n    test_capitalize();\n    test_range();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 452 Oct 30 10:00 utils.js",
      "-rw-r--r-- 1 user user 1486 Oct 30 10:00 test_utils.js",
      "$ node test_utils.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_utils.js",
        "description": "Tests must run successfully (exit code 0)"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "utils"
    }
  },
  {
    "id": "prompt_000264",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Find all Python files (*.py) in the project, excluding the 'tests' directory, and create a 'python_files.txt' listing with their full paths. Also create 'file_count.txt' with the total count.",
    "files": [
      {
        "path": "src/main.py",
        "content": "# Main module\nprint('main')",
        "is_test": false
      },
      {
        "path": "src/utils.py",
        "content": "# Utils\ndef helper(): pass",
        "is_test": false
      },
      {
        "path": "tests/test_main.py",
        "content": "# Tests\nimport main",
        "is_test": false
      },
      {
        "path": "tests/test_utils.py",
        "content": "# Tests\nimport utils",
        "is_test": false
      },
      {
        "path": "docs/README.md",
        "content": "# Documentation",
        "is_test": false
      },
      {
        "path": "docs/API.md",
        "content": "# API Docs",
        "is_test": false
      },
      {
        "path": ".gitignore",
        "content": "*.pyc\n__pycache__/",
        "is_test": false
      },
      {
        "path": "setup.py",
        "content": "from setuptools import setup",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "tree ."
    ],
    "expected_commands": 8,
    "verification_rules": [
      {
        "type": "text_match",
        "target": "python_files.txt",
        "description": "Python files listed"
      }
    ],
    "metadata": {
      "scenario_type": "directory_tree",
      "command_focus": "find,wc,grep",
      "solution_steps": [
        "List all directories: find . -type d",
        "Find all .py files: find . -name '*.py'",
        "Exclude tests: find . -name '*.py' -not -path './tests/*'",
        "Save to file: find . -name '*.py' -not -path './tests/*' > python_files.txt",
        "Count files: find . -name '*.py' -not -path './tests/*' | wc -l",
        "Save count: find . -name '*.py' -not -path './tests/*' | wc -l > file_count.txt",
        "Verify list: cat python_files.txt",
        "Verify count: cat file_count.txt"
      ]
    }
  },
  {
    "id": "prompt_000822",
    "difficulty": "medium",
    "language": "python",
    "task_description": "The python project has failing tests. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: data_processor.py, test_data_processor.py",
    "files": [
      {
        "path": "data_processor.py",
        "content": "\"\"\"Data processing utilities.\"\"\"\n\ndef filter_positive(numbers)\n    \"\"\"Filter positive numbers from a list.\"\"\"\n    return [n for n in numbers if n > 0]\n\ndef sum_even(numbers):\n    \"\"\"Sum all even numbers in a list.\"\"\"\n    return sum(n for n in numbers if n % 2 == 0)\n\ndef find_max(numbers):\n    \"\"\"Find maximum value in a list.\"\"\"\n    if not numbers:\n        return None\n    return max(numbers)\n\ndef average(numbers):\n    \"\"\"Calculate average of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\ndef remove_duplicates(items):\n    \"\"\"Remove duplicates while preserving order.\"\"\"\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n",
        "is_test": false
      },
      {
        "path": "test_data_processor.py",
        "content": "\"\"\"Tests for data processor.\"\"\"\n\nfrom data_processor import filter_positive, sum_even, find_max, average, remove_duplicates\n\ndef test_filter_positive():\n    assert filter_positive([1, -2, 3, -4, 5]) == [1, 3, 5]\n    assert filter_positive([-1, -2, -3]) == []\n    assert filter_positive([]) == []\n\ndef test_sum_even():\n    assert sum_even([1, 2, 3, 4, 5, 6]) == 12\n    assert sum_even([1, 3, 5]) == 0\n\ndef test_find_max():\n    assert find_max([1, 5, 3, 9, 2]) == 9\n    assert find_max([-5, -1, -10]) == -1\n    assert find_max([]) is None\n\ndef test_average():\n    assert average([1, 2, 3, 4, 5]) == 3.0\n    assert average([10]) == 10.0\n    assert average([]) == 0\n\ndef test_remove_duplicates():\n    assert remove_duplicates([1, 2, 2, 3, 1, 4]) == [1, 2, 3, 4]\n    assert remove_duplicates([]) == []\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 778 Oct 30 10:00 data_processor.py",
      "-rw-r--r-- 1 user user 797 Oct 30 10:00 test_data_processor.py"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "test",
        "target": "test_data_processor.py",
        "description": "All data processor tests must pass"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing colon on line 3"
      ],
      "scenario_type": "data_processor"
    }
  },
  {
    "id": "prompt_000147",
    "difficulty": "hard",
    "language": "javascript",
    "task_description": "Fix the broken javascript code. Multiple issues may need to be resolved. Carefully examine the test output and trace through the code. The project has these files: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) {\n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 513 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js",
      "$ node test_validators.js",
      "Test failed: ..."
    ],
    "expected_commands": 9,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000229",
    "difficulty": "very_hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000237",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  },
  {
    "id": "prompt_000481",
    "difficulty": "easy",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000810",
    "difficulty": "hard",
    "language": "python",
    "task_description": "Create a symbolic link named 'config.ini' that points to 'config.dev.ini'. Then verify the link works by running the app and checking that it uses the dev configuration.",
    "files": [
      {
        "path": "config.dev.ini",
        "content": "[database]\nhost=localhost\nport=5432\nname=dev_db\n",
        "is_test": false
      },
      {
        "path": "config.prod.ini",
        "content": "[database]\nhost=prod.server.com\nport=5432\nname=prod_db\n",
        "is_test": false
      },
      {
        "path": "app.py",
        "content": "import configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')  # Reads the symlink\n\nprint(f\"Database: {config['database']['name']}\")\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls *.ini"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "app.py",
        "description": "App runs successfully"
      }
    ],
    "metadata": {
      "scenario_type": "symlinks",
      "command_focus": "ln,readlink,ls",
      "solution_steps": [
        "Create symlink: ln -s config.dev.ini config.ini",
        "Verify link: ls -l config.ini",
        "Check target: readlink config.ini",
        "Test app: python3 app.py",
        "Switch to prod: rm config.ini && ln -s config.prod.ini config.ini",
        "Verify switch: readlink config.ini"
      ]
    }
  },
  {
    "id": "prompt_000899",
    "difficulty": "medium",
    "language": "javascript",
    "task_description": "There's a bug in the javascript code that needs fixing. You'll need to explore the codebase to find the issue. Check the test failures for clues. Project structure: validators.js, test_validators.js",
    "files": [
      {
        "path": "validators.js",
        "content": "// Validation functions\n\nfunction isValidEmail(email) \n    const regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return regex.test(email);\n}\n\nfunction isValidPhone(phone) {\n    const regex = /^\\d{3}-\\d{3}-\\d{4}$/;\n    return regex.test(phone);\n}\n\nfunction isValidPassword(password) {\n    return password.length >= 8;\n}\n\nfunction isValidUsername(username) {\n    const regex = /^[a-zA-Z0-9_]{3,20}$/;\n    return regex.test(username);\n}\n\nmodule.exports = { isValidEmail, isValidPhone, isValidPassword, isValidUsername };\n",
        "is_test": false
      },
      {
        "path": "test_validators.js",
        "content": "// Tests for validators\n\nconst { isValidEmail, isValidPhone, isValidPassword, isValidUsername } = require('./validators');\n\nfunction assertEquals(actual, expected, message) {\n    if (actual !== expected) {\n        throw new Error(`${message}: expected ${expected}, got ${actual}`);\n    }\n}\n\nfunction test_isValidEmail() {\n    assertEquals(isValidEmail(\"user@example.com\"), true, \"valid email\");\n    assertEquals(isValidEmail(\"invalid\"), false, \"invalid email\");\n    console.log(\"\u2713 test_isValidEmail passed\");\n}\n\nfunction test_isValidPhone() {\n    assertEquals(isValidPhone(\"123-456-7890\"), true, \"valid phone\");\n    assertEquals(isValidPhone(\"1234567890\"), false, \"invalid phone\");\n    console.log(\"\u2713 test_isValidPhone passed\");\n}\n\nfunction test_isValidPassword() {\n    assertEquals(isValidPassword(\"password123\"), true, \"valid password\");\n    assertEquals(isValidPassword(\"short\"), false, \"invalid password\");\n    console.log(\"\u2713 test_isValidPassword passed\");\n}\n\nfunction test_isValidUsername() {\n    assertEquals(isValidUsername(\"user_123\"), true, \"valid username\");\n    assertEquals(isValidUsername(\"ab\"), false, \"too short username\");\n    console.log(\"\u2713 test_isValidUsername passed\");\n}\n\n// Run all tests\ntry {\n    test_isValidEmail();\n    test_isValidPhone();\n    test_isValidPassword();\n    test_isValidUsername();\n    console.log(\"All tests passed!\");\n} catch (e) {\n    console.error(\"Test failed:\", e.message);\n    process.exit(1);\n}\n",
        "is_test": true
      }
    ],
    "cli_history": [
      "$ ls -la",
      "-rw-r--r-- 1 user user 512 Oct 30 10:00 validators.js",
      "-rw-r--r-- 1 user user 1442 Oct 30 10:00 test_validators.js"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "execution",
        "target": "test_validators.js",
        "description": "Tests must run successfully"
      }
    ],
    "metadata": {
      "bugs": [
        "Missing opening brace on line 3"
      ],
      "scenario_type": "validators"
    }
  },
  {
    "id": "prompt_000727",
    "difficulty": "medium",
    "language": "python",
    "task_description": "Create a compressed backup archive named 'backup.tar.gz' containing all Python files in the 'src' directory and the config.ini file. Then verify the archive contents.",
    "files": [
      {
        "path": "src/main.py",
        "content": "def main():\n    print(\"File 1\")\n",
        "is_test": false
      },
      {
        "path": "src/helper.py",
        "content": "def helper():\n    return \"Helper\"\n",
        "is_test": false
      },
      {
        "path": "config.ini",
        "content": "[settings]\ndebug=true\nport=8000\n",
        "is_test": false
      }
    ],
    "cli_history": [
      "ls",
      "ls src/"
    ],
    "expected_commands": 6,
    "verification_rules": [
      {
        "type": "text_match",
        "target": ".",
        "description": "Archive created"
      }
    ],
    "metadata": {
      "scenario_type": "archive",
      "command_focus": "tar,gzip,find",
      "solution_steps": [
        "List files: ls -R",
        "Create archive: tar -czf backup.tar.gz src/ config.ini",
        "Or find and archive: find . -name '*.py' -o -name '*.ini' | tar -czf backup.tar.gz -T -",
        "List archive contents: tar -tzf backup.tar.gz",
        "Verify: ls -lh backup.tar.gz",
        "Extract to test (optional): tar -xzf backup.tar.gz -C /tmp/test"
      ]
    }
  }
]